# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EbsPKJkefrbsj7Iyd3sPEETotA6_zQhP
"""

#for model

import numpy as np
import seaborn as sns
import random
import os
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.mcd import MCD
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import LocalOutlierFactor
from sklearn.mixture import GaussianMixture
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,losses,models
from tensorflow.keras.models import Model, load_model
from keras.callbacks import EarlyStopping, ModelCheckpoint

def MinimumCovarianceDeterminant(train=None,test=None):
  model_mcd = "MCD"
  model_mcd = MCD(contamination=0.01,random_state=42)
  model_mcd.fit(train)
  return model_mcd

def OCSVM(train=None,test=None):
  model_ocsvm = "OCSVM"
  model_ocsvm = OneClassSVM(nu=0.01)
  model_ocsvm.fit(train)
  return model_ocsvm

def LOF(train=None,test=None):
  model_lof = "LOF"
  model_lof = LocalOutlierFactor(contamination=0.01,novelty=True)
  model_lof.fit(train)
  return model_lof

def IForest(train=None,test=None):
  model_iforest = "IForest"
  model_iforest = IsolationForest(contamination = 0.01)
  model_iforest.fit(train)
  return model_iforest

def GausianMixtureModel(train=None,test=None):
  model_gmm = "GMM"
  lowest_bic = np.infty
  bic = []
  n_components_range = range(1,15)
  cv_types = ["spherical", "tied", "diag", "full"]
  for cv_type in cv_types:
      for n_components in n_components_range:
          gmm = GaussianMixture(
              n_components=n_components, covariance_type=cv_type
          )
          gmm.fit(train)
          bic.append(gmm.bic(train))
          if bic[-1] < lowest_bic:
              lowest_bic = bic[-1]
              model_gmm = gmm
  if model_gmm.converged_:
    print('gmm model is converged')
  return model_gmm

def AutoEncoder(train=None,test=None):
  model = "AE"
  def my_seed_everywhere(seed: int = 2):
    random.seed(seed) # random
    np.random.seed(seed) # np
    os.environ["PYTHONHASHSEED"] = str(seed) # os
    tf.random.set_seed(seed) # tensorflow

  my_seed = 2
  my_seed_everywhere(my_seed)

  input_dim=train.shape[1]
  #tf.random.set_seed(2)
  initializer = tf.keras.initializers.HeNormal(seed=2)
  momentum = 0.9
  early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=10)
  check_point = ModelCheckpoint('best_ae_model.h5',monitor='val_loss',mode='min',save_best_only=True)


  encoder=models.Sequential([
      #input layer
      layers.InputLayer(input_shape=input_dim),

      layers.Dense(64,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(32,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(16,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(8,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(4,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
  ])

  decoder=models.Sequential([ 
      
      layers.Dense(4,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(8,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(16,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(32,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      layers.Dense(64,kernel_initializer=initializer),
      layers.BatchNormalization(momentum=momentum),
      layers.ReLU(),
      
      #output layer
      layers.Dense(input_dim,kernel_initializer=initializer),
  ])

  model=models.Sequential([
      encoder,decoder
  ])

  model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.03), 
                      loss=keras.losses.MeanSquaredError())

  history = model.fit(
      train,train,
      shuffle=True,
      epochs=100,
      batch_size=128,
      validation_split = 0.3,
      callbacks = [early_stopping, check_point],
      verbose = 0
  )




  return model

